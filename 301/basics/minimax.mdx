import { Callout } from 'nextra/components'

# Minimaxity

Let $X_{1},\ldots,X_{n}\overset{\text{i.i.d.}}{\sim}P_{\theta}$, and let $\widehat{\theta}=\widehat{\theta}(X_{1},\ldots,X_{n})$ be an estimator of $\theta$. Let $L(\widehat{\theta},\theta)$ be a loss function. 

<details open>
<summary>Definition</summary>

We say $\widehat\theta$ is a minimax estimator if 

$$
\widehat{\theta}=\argmin_{\widehat{\theta}}\sup_{\theta\in\Theta}R(\widehat{\theta},\theta).
$$

</details>

## Finding minimax estimators

There is a strong connection between Bayes and minimax estimators. It turns out that any estimator whose maximum risk is equal to the Bayes risk for some prior is minimax. In fact, we have the following slightly stronger result.

<details open>
<summary>Theorem</summary>

If there exists a sequence priors $(\pi_m)$ such that $\widehat{\theta}$ satisfies 

$$
\sup_{\theta\in\Theta}R(\widehat{\theta},\theta)=\lim_{m\to\infty} \inf_{\widetilde{\theta}}\int R(\widetilde{\theta},\theta)\pi_m(\theta)\,d\theta,
$$

then $\widehat{\theta}$ is minimax. 
</details>

<details>
<summary>Proof</summary>

For all $\widetilde{\theta}$, we have

$$
\sup_{\theta\in\Theta}R(\widetilde{\theta},\theta)\geq\int R(\widetilde{\theta},\theta)\pi_m(\theta)\,d\theta\geq\inf_{\widetilde{\theta}}\int R(\widetilde{\theta},\theta)\pi_m(\theta)\,d\theta.
$$

Taking the limit as $m \to \infty$ gives $\sup_{\theta\in\Theta}R(\widetilde{\theta},\theta) \geq \sup_{\theta\in\Theta}R(\widehat{\theta},\theta).$
</details>

From this, we see that any Bayes estimator with constant risk is minimax. 

<details open>
<summary>Corollary</summary>

If $\widehat{\theta}=\widehat{\theta}_{\pi}$ for some $\pi$ and $R(\widehat{\theta},\theta)$ is constant over $\theta\in\Theta$, then $\widehat{\theta}$ is minimax. 
</details>


<details>
<summary>Proof</summary>

Since $R(\widehat{\theta},\theta)$ is constant, we have

$$
\sup_{\theta\in\Theta}R(\widehat{\theta},\theta)=\int R(\widehat{\theta},\theta)\pi(\theta)\,d\theta=\inf_{\widetilde{\theta}}\int R(\widetilde{\theta},\theta)\pi(\theta)\,d\theta,
$$

so by the theorem we have that $\widehat{\theta}$ is minimax. 

</details>

This fact is usually the first line of attack when finding minimax estimators. 

<details open>
<summary>Example</summary>

Let $X_{1},\ldots,X_{n}\overset{\text{i.i.d.}}{\sim}\text{Ber}(p)$, and let $L(\hat{p},p)=(\hat{p}-p)^{2}$. Recall that for the prior $\pi\sim\text{Beta}(\alpha,\beta)$ we have the Bayes estimator

$$
\hat{p}=\mathbb{E}(p|X_{1},\ldots,X_{n})=\frac{\sum_{i=1}^{n}X_{i}+\alpha}{n+\alpha+\beta}.
$$

The risk is 

$$
\begin{aligned}
R(\hat{p},p) &= \mathbb{E}_{p}(\hat{p}-p)^{2} \\
&=\left(\frac{n}{n+\alpha+\beta}\right)^{2}\frac{p(1-p)}{n}+\left(\frac{\alpha+\beta}{\alpha+\beta+n}\right)^{2}\left(\frac{\alpha}{\alpha+\beta}-p\right)^{2},
\end{aligned}
$$

which can be written as 

$$
\begin{aligned}
&\left(\left(\frac{\alpha+\beta}{n+\alpha+\beta}\right)^{2}-\frac{1}{n}\left(\frac{n}{n+\alpha+\beta}\right)^{2}\right)p^{2} \\
&\qquad \qquad +\left(\frac{1}{n}\left(\frac{n}{n+\alpha+\beta}\right)^{2}-\left(\frac{\alpha+\beta}{n+\alpha+\beta}\right)^{2}\frac{2\alpha}{\alpha+\beta}\right)p \\
&\qquad\qquad\qquad\qquad+\left(\frac{\alpha+\beta}{n+\alpha+\beta}\right)^{2}\left(\frac{\alpha}{\alpha+\beta}\right)^{2}
\end{aligned}
$$

and notice if $\alpha=\beta=\frac{\sqrt{n}}{2}$, then the coefficients of the $p$ and $p^{2}$ terms vanish, and the risk becomes

$$
R(\hat{p},p)=\frac{1}{4(\sqrt{n}+1)^{2}}
$$

which is constant with respect to $p$. Therefore, 

$$
\hat{p}_{\text{minimax}}=\frac{\sum_{i=1}^{n}X_{i}+\frac{\sqrt{n}}{2}}{n+\sqrt{n}}
$$

is a minimax estimator. How does this estimator compare to the maximum likelihood estimator $\hat{p} _ {\text{MLE}}=\overline{X}$? The risk function for the MLE estimator is 

$$
R(\hat{p}_{\text{MLE}},p)=\mathbb{E}_{p}(\hat{p}_{\text{MLE}}-p)^{2}=\frac{p(1-p)}{n}.
$$

There are some situations where the MLE has lower risk, for example when $p $is close to 0 or 1. However, if we consider the worst case, we find that

$$
\sup_{p\in(0,1)}R(\hat{p}_{\text{MLE}},p)=\frac{1}{4n}
$$

which is larger than 

$$
\sup_{p\in(0,1)}R(\hat{p}_{\text{minimax}},p)=\frac{1}{4(\sqrt{n}+1)^{2}}.
$$ 
</details>


The choice of loss function is important. Let us demonstrate this with another example. 

<details open>
<summary>Example</summary>

Let $X_{1},\ldots,X_{n}\overset{\text{i.i.d.}}{\sim}\text{Ber}(p)$, and let $L(\hat{p},p)=\frac{(\hat{p}-p)^{2}}{p(1-p)}$. For the prior $\pi(p)=1$, the Bayes estimator is

$$
\begin{aligned}
\hat{p}(X) &= \argmin_{a} \int \frac{(a-p)^{2}}{p(1-p)} \pi(p|X)\,dp \\ 
	&=\argmin_{a}\int(a-p)^{2}\frac{\pi(p|X)}{p(1-p)}\,dp,
\end{aligned}
$$
where 

$$
\frac{\pi(p|X)}{p(1-p)}\propto p^{\sum_{i=1}^{n}X_{i}-1}(1-p)^{\sum_{i=1}^{n}(X_{i}-1)-1}=\text{Beta}\left(\sum_{i=1}^{n}X_{i},\sum_{i=1}^{n}(1-X_{i})\right),
$$

so

$$
\hat{p}(X)=\frac{\sum_{i=1}^{n}X_{i}}{\sum_{i=1}^{n}X_{i}+\sum_{i=1}^{n}(1-X_{i})}=\overline{X}=\hat{p}_{\text{MLE}}.
$$

The risk is 

$$
R(\hat{p},p) =\frac{\mathbb{E}_{p}(\hat{p}-p)^{2}}{p(1-p)}=\frac{1}{n}
$$

which is constant. Therefore, $\overline{X}$ is a Bayes estimator (which cannot be the case with the squared loss function because only biased estimators could be Bayes), and it is also a minimax estimator. 
</details>

Not all minimax estimators are Bayes, as the following example shows.

<details open>
<summary>Example</summary>

Let $X_{1},\ldots,X_{n}\overset{\text{i.i.d.}}{\sim}N(\mu,\sigma^{2})$ where $\mu\in\mathbb{R}$ is the parameter to be estimated. Consider the loss function $L(\hat{\mu},\mu)=(\hat{\mu}-\mu)^{2}$. Then the estimator $\overline{X}$ cannot be Bayes as it is unbiased, but is it minimax? This is true intuitively because the risk $R(\overline{X},\mu)=\mathbb{E} _ {\mu}(\overline{X}-\mu)^{2}=\frac{\sigma^{2}}{n}$ is constant. To prove this formally, consider the prior $\pi\sim N(0,\tau^{2})$. Then the Bayes estimator for this prior is

$$
\hat{\mu}_{\text{Bayes}}=\mathbb{E}(\mu|X)=\frac{\frac{n}{\sigma^{2}}}{\frac{1}{\tau^{2}}+\frac{n}{\sigma^{2}}}\overline{X},
$$

and the risk is

$$
\begin{aligned}
R(\hat{\mu}_{\text{Bayes}},\mu) &= \text{Var}(\hat{\mu}_{\text{Bayes}})+(\mathbb{E}\hat{\mu}_{\text{Bayes}}-\mu)^{2} \\ 
	&=\left(\frac{\frac{n}{\sigma^{2}}}{\frac{1}{\tau^{2}}+\frac{n}{\sigma^{2}}}\right)^{2}\frac{\sigma^{2}}{n}+\left(\frac{\frac{1}{\tau^{2}}}{\frac{1}{\tau^{2}}+\frac{n}{\sigma^{2}}}\right)^{2}\mu^{2}.
\end{aligned}
$$

The Bayes risk is

$$
\int R(\hat{\mu}_{\text{Bayes}},\mu)\pi(\mu)\,d\mu=\left(\frac{\frac{n}{\sigma^{2}}}{\frac{1}{\tau^{2}}+\frac{n}{\sigma^{2}}}\right)^{2}\frac{\sigma^{2}}{n}+\left(\frac{\frac{1}{\tau^{2}}}{\frac{1}{\tau^{2}}+\frac{n}{\sigma^{2}}}\right)^{2}\tau^{2}=\frac{1}{\frac{1}{\tau^{2}}+\frac{n}{\sigma^{2}}}.
$$

Letting $\tau\to\infty$, we get $R(\overline{X},\mu) = \frac{\sigma^{2}}{n},$ so $\overline{X}$ is minimax.
</details>

## Admissibility of normal sample mean

Are minimax estimators always admissible? Suppose that we have observations $X_{1},\ldots,X_{n}\overset{\mathrm{i.i.d.}}{\sim}N(\theta,I_p), \theta\in\mathbb{R}^p$ and consider the squared loss function $L(\hat{\theta},\theta)=\Vert \hat{\theta}-\theta\Vert^{2}.$ We know that $\overline{X}$ is a minimax estimator. In dimension $p = 1,$ it can be shown that the sample mean is indeed admissible.

<details open>
<summary>Theorem</summary>

If $p = 1,$ then $\overline{X}$ is admissible.
</details>

<details>
<summary>Proof</summary>

The proof is by contradiction. Suppose that $\hat{\theta}=\overline{X}$ is inadmissible, so that there exists $\tilde{\theta}$ such that $R(\tilde{\theta},\theta)\leq\frac{1}{n}$ for all $\theta\in\mathbb{R}$, and $R(\tilde{\theta},\theta_{0})<\frac{1}{n}$ for some $\theta_{0}\in\mathbb{R}$. Then there exists $\epsilon>0$ and $(a,b)\ni\theta_{0}$ such that $R(\tilde{\theta},\theta_{0})<\frac{1}{n}-\epsilon$. Consider $\pi_{m}=N(0,m)$. Then the Bayes risk is

$$
\int_{\mathbb{R}}R(\hat{\theta}_{\pi_{m}},\theta)\pi_{m}(\theta)\,d\theta=\frac{1}{n+\frac{1}{m}},
$$

and we have

$$
\frac{1}{n}-\int_{\mathbb{R}}R(\hat{\theta}_{\pi_{m}},\theta)\pi_{m}(\theta)\,d\theta=\frac{1}{n}\cdot\frac{\frac{1}{m}}{n+\frac{1}{m}}\asymp\frac{1}{m}.
$$

Then

$$
\begin{aligned}
\frac{1}{n}-\int_{\mathbb{R}}R(\tilde{\theta},\theta) \pi_{m} (\theta)\,d\theta	&= \int_{(a,b)}\left(\frac{1}{n}-R(\tilde{\theta},\theta)\right)\pi_{m}(\theta)\,d\theta \\ 
&\qquad \qquad +\int_{\mathbb{R}\backslash(a,b)}\left(\frac{1}{n}-R(\tilde{\theta},\theta)\right)\pi_{m}(\theta)\,d\theta \\
	&\geq\epsilon\int_{(a,b)}\pi_{m}(\theta)\,d\theta \\
	&=\epsilon \mathbb{P}\left(\frac{a}{\sqrt{m}} < N(0,1) < \frac{b}{\sqrt{m}}\right) \\
	&=\epsilon\int_{\frac{a}{\sqrt{m}}}^{\frac{b}{\sqrt{m}}}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}\,dx \\
	&\asymp\frac{1}{\sqrt{m}}.
\end{aligned}
$$

So there exists $m$ sufficiently large such that

$$
\frac{1}{n}-\int_{\mathbb{R}}R(\hat{\theta}_{\pi_{m}},\theta)\pi(\theta)\,d\theta<\frac{1}{n}-\int_{\mathbb{R}}R(\tilde{\theta},\theta)\pi_{m}(\theta)\,d\theta,
$$

which is equivalent to 

$$
\int_{\mathbb{R}}R(\tilde{\theta},\theta)\pi(\theta)\,d\theta<\int_{\mathbb{R}}R(\hat{\theta}_{\pi_{m}},\theta)\pi_{m}(\theta)\,d\theta.
$$

This is a contradiction as $\hat{\theta} _ {\pi_{m}}$ achieves the smallest Bayes risk by definition. 

</details>


Admissibility of $\overline{X} $ in higher dimensions is much more difficult. For $p=2$, admissibility of $\overline{X}$ was proved by Stein. The proof involves constructing much more complex priors. Surprisingly, $\overline{X}$ is no longer admissible if $p \geq 3$. Brown proved in 1971 that this phenomenon (aptly named Stein's paradox) is linked to the transience of the symmetric random walk in $p \geq 3$. 
