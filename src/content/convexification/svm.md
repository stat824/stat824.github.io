# Support vector machines

Support vector machine (SVM) method is a particular case of the technique of convexification corresponding to the hinge loss function $\psi(x)=(1+x)_{+}.$ The set of candidate functions has the form $\mathcal{F}=\{f:\Vert f\Vert_{\mathcal{W}}\leq\lambda\},$ where $\Vert\cdot\Vert_{\mathcal{W}}$ is the norm in a Hilbert space $\mathcal{W},$ known as the reproducing kernel Hilbert space (RKHS). To define the RKHS, we will need a few definitions.

<details open>
<summary>Definition</summary>

Let $\mathcal{X}$ be a nonempty set. A function $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is called a positive definite kernel if it is symmetric (i.e., $k(x,x')=k(x',x)$ for all $x,x'\in\mathcal{X}$) and satisfies, for any $N\in\mathbb{N},$ that for any $N$-tuple $(x_{1},\ldots,x_{N})\in\mathcal{X}^{N}$ and any vector $(a_{1},\ldots,a_{N})\in\mathbb{R}^{N},$ we have

$$
\sum_{i=1}^{N}\sum_{j=1}^{N}a_{i}a_{j}k(x_{i},x_{j})\geq0.
$$

</details>



It is easy to see that, for $\mathcal{X}=\mathbb{R}^{d},$ the following functions are positive definite kernels:

* the bilinear kernel $k(x,x')=\langle x,x'\rangle_{\mathbb{R}^{d}},$

* the Gaussian kernel $k(x,x')=\exp(-\Vert x-x'\Vert_{2}^{2}/2\sigma^{2})$ for $\sigma>0.$

<details open>
<summary>Definition</summary>

Let $\mathcal{X}$ be any set and $\mathcal{W}$ be a Hilbert space of functions from $\mathcal{X}$ to $\mathbb{R}.$ A function $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is called a reproducing kernel of $\mathcal{W}$ if:

1. For all $x\in\mathcal{X},$ the functions $k(x,\cdot)$ belong to $\mathcal{W}.$

2. For all $x\in\mathcal{X}$ and all $f\in\mathcal{W},$ we have the reproducing property:

   $$
   f(x)=\langle f,k(x,\cdot)\rangle_{\mathcal{W}}.
   $$

If a reproducing kernel exists, we call $\mathcal{W}$ a reproducing kernel Hilbert space (RKHS).
</details>


A general property, whose proof is beyond the scope of this course, is that a kernel is positive definite if and only if there is a Hilbert space $\mathcal{W}$ for which it is reproducing. 

<details open>
<summary>Example</summary>

Suppose that $\mathcal{W}$ is of finite dimension generated by the functions $\phi_{1},\ldots,\phi_{M},$ where $\phi_{j}:[0,1]\to\mathbb{R}$ are $M$ orthonormal elements of $L^{2}([0,1]).$ By equipping $\mathcal{W}$ with the usual scalar product of $L^{2}([0,1]),$ we immediately see that

$$
k(x,x')=\sum_{j=1}^{M}\phi_{j}(x)\phi_{j}(x'),
$$

is a reproducing kernel of $\mathcal{W}.$
</details>


<details open>
<summary> Proposition </summary>

Let $\mathcal{X}$ be any set, $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ a positive definite kernel, and $\mathcal{W}$ the associated RKHS. Then $\langle k(x,\cdot),k(x',\cdot)\rangle_{\mathcal{W}}=k(x,x')$ and $\sup_{f\in\mathcal{W}}\frac{|f(x)|}{\Vert f\Vert_{\mathcal{W}}}=\sqrt{k(x,x)}.$
</details>


<details>
<summary>Proof</summary>

The first identity is a special case of the reproducing property. The second follows from

$$
\sup_{f\in\mathcal{W}}\frac{|f(x)|}{\Vert f\Vert_{\mathcal{W}}}=\sup_{f\in\mathcal{W}}\frac{|\langle f,k(x,\cdot)\rangle_{\mathcal{W}}|}{\Vert f\Vert_{\mathcal{W}}}=\Vert k(x,\cdot)\Vert_{\mathcal{W}}=\sqrt{k(x,x)}.
$$
</details>



Letâ€™s now fix an RKHS $\mathcal{W}.$ Following the general methodology of convexification, one can focus on classifiers resulting from the minimization problem

$$
\widehat{f}_{n,\phi}=\argmin_{\Vert f\Vert_{\mathcal{W}}\leq\lambda}R_{n,\phi}(f)
$$

for a constant $\lambda>0.$ The method of Lagrange multipliers then leads us to the penalized problem

$$
\widehat{f}_{n,\phi}=\argmin_{f\in\mathcal{W}}\left(R_{n,\phi}(f)+\lambda'\Vert f\Vert_{\mathcal{W}}^{2}\right)
$$

for a constant $\lambda'>0.$ At first glance, this problem seems complicated, as minimization extends over the Hilbert space $\mathcal{W}$ which can be infinite-dimensional. However, the fact that $\mathcal{W}$ is an RKHS allows to reduce this to a minimization problem in finite dimension. This remarkable property is known as the representer theorem.

<details open>
<summary> Representer Theorem </summary>

Let $\mathcal{X}$ be any set, $k$ a positive definite kernel on $\mathcal{X},$ and $\mathcal{W}$ the associated RKHS. Let $\Omega:\mathbb{R}\to\mathbb{R}$ be a strictly increasing function defined on $\mathcal{X}.$ Consider a function $G:\mathbb{R}^{n}\to\mathbb{R}.$ Then, for any set of $n$ elements $x_{1},\ldots,x_{n},$ every solution to the minimization problem

$$
\min_{f\in\mathcal{W}}\left\{ G(f(x_{1}),\ldots,f(x_{n}))+\Omega(\Vert f\Vert_{\mathcal{W}})\right\}
$$

admits the representation

$$
f(x)=\sum_{i=1}^{n}\theta_{i}k(x_{i},x)\quad\text{for all }x\in\mathcal{X}
$$

where $\theta_{1},\ldots,\theta_{n}\in\mathbb{R}. $
</details>

The representer theorem gives that the solution to the penalized problem has form:

$$
\widehat{f}_{n,\varphi} = \sum_{i=1}^n \widehat{\theta}_i k(X_i,x)
$$

with $\widehat{\theta} = (\widehat{\theta}_1,...,\widehat{\theta}_n)$ verifying

$$
\widehat{\theta} = \argmin_{\theta\in\mathbb{R}^n} \left\{ \frac{1}{n} \sum_{i=1}^n \varphi\left( - Y_i \sum_{j=1}^n \theta_j k(X_j, X_i) \right) + \lambda\theta^T K \theta \right\}
$$

where $K$ is the matrix $K = (k(X_i,X_j))_{i,j=1,...,n}.$ This reduces the problem to a convex optimization problem in finite dimension. The most famous particular case of is that of support vector machines (SVM), which corresponds to the loss function $\varphi(x) = (1 + x)_+.$ In this case, we can show that the solution of problem benefits from the parsimony property: only a fraction of the solution coefficients $\widehat{\theta}_i$ close to the classification boundary are non-zero. This conclusion means that only the $X_i$ close to the classification boundary determined by the $\widehat{\theta}_i$ are important for determining the performance of the classifier.